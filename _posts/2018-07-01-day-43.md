---
title: Day 43
date: 2018-07-01 00:00:00 +00:00
layout: post
description: Only if One of Them Turned Into a Cat
---
## Research
### Top 20 Deep Learning Papers
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013509807226212352">
    <blockquote placeholder><p lang="en" dir="ltr">Top 20 Deep Learning Papers, 2018 Edition <a href="https://t.co/xCjXFBc12v">https://t.co/xCjXFBc12v</a> <a href="https://t.co/6FAc4DLxcw">pic.twitter.com/6FAc4DLxcw</a></p>&mdash; KDnuggets (@kdnuggets) <a href="https://twitter.com/kdnuggets/status/1013509807226212352?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote>
</amp-twitter>

### Gradient Acceleration in Activation Functions
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013578783993954304">
    <blockquote placeholder><p lang="en" dir="ltr">While DropOut has been considered to prevent the co-adaption among hidden neurons to regularize the model, DropOut actually makes gradient flows even when the activation functions are saturated and helps the optimization converges to the flat minima.  <a href="https://t.co/YEH33wWqRZ">https://t.co/YEH33wWqRZ</a></p>&mdash; Daisuke Okanohara (@hillbig) <a href="https://twitter.com/hillbig/status/1013578783993954304?ref_src=twsrc%5Etfw">July 2, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013666174725513216">
    <blockquote placeholder><p lang="en" dir="ltr">Dropout has been a standard technique for years, but we‚Äôre still finding new ways to interpret it. <br><br>I tend to think of new techniques/architectures as an advancement in either regularization or optimization/gradient flow, but sometimes it‚Äôs difficult to tell which one it is. <a href="https://t.co/74lt0VtExG">https://t.co/74lt0VtExG</a></p>&mdash; Denny Britz (@dennybritz) <a href="https://twitter.com/dennybritz/status/1013666174725513216?ref_src=twsrc%5Etfw">July 2, 2018</a></blockquote>
</amp-twitter>

## Tutorials / Reviews
## Fasti.ai: How to Start?
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013433777174040577">
    <blockquote placeholder><p lang="en" dir="ltr">Ask ‚ÄúHowTo start with <a href="https://twitter.com/fastdotai?ref_src=twsrc%5Etfw">@fastdotai</a>?‚Äù may seem incongruous. Just watch the <a href="https://twitter.com/hashtag/videos?src=hash&amp;ref_src=twsrc%5Etfw">#videos</a>, right? No. On the basis of my double experience with <a href="https://twitter.com/hashtag/Fastai?src=hash&amp;ref_src=twsrc%5Etfw">#Fastai</a> since 2017 (student &amp; instructor in <a href="https://twitter.com/hashtag/Brasilia?src=hash&amp;ref_src=twsrc%5Etfw">#Brasilia</a>), I publish this start-up guide for the new learners. <a href="https://t.co/9kvs4BPygK">https://t.co/9kvs4BPygK</a> <a href="https://twitter.com/jeremyphoward?ref_src=twsrc%5Etfw">@jeremyphoward</a></p>&mdash; Pierre Guillou (@pierre_guillou) <a href="https://twitter.com/pierre_guillou/status/1013433777174040577?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote>
</amp-twitter>

### Abstract Art with ML
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013377451240624129">
    <blockquote placeholder><p lang="en" dir="ltr">Abstract Art with ML using Compositional Pattern Producing Networks<br>By <a href="https://twitter.com/janhuenermann?ref_src=twsrc%5Etfw">@janhuenermann</a><a href="https://t.co/RJAeo50u3w">https://t.co/RJAeo50u3w</a> <a href="https://t.co/cUcdu6KOqV">pic.twitter.com/cUcdu6KOqV</a></p>&mdash; ML Review (@ml_review) <a href="https://twitter.com/ml_review/status/1013377451240624129?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote>
</amp-twitter>

### Design Space for #dataviz
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013384282906267649">
    <blockquote placeholder><p lang="en" dir="ltr">üëç Sunday-morning read‚Ä¶<br>üìÑ &quot;Using Typography to Expand the Design Space of Data Visualization&quot; by <a href="https://twitter.com/rkbrath?ref_src=twsrc%5Etfw">@rkbrath</a> &amp; Ebad Banissi<a href="https://t.co/P6cB0Cz3co">https://t.co/P6cB0Cz3co</a> <a href="https://twitter.com/hashtag/dataviz?src=hash&amp;ref_src=twsrc%5Etfw">#dataviz</a> <a href="https://twitter.com/hashtag/infovis?src=hash&amp;ref_src=twsrc%5Etfw">#infovis</a> <a href="https://t.co/v8ac5ds22V">pic.twitter.com/v8ac5ds22V</a></p>&mdash; Mara Averick (@dataandme) <a href="https://twitter.com/dataandme/status/1013384282906267649?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote>
</amp-twitter>

### French Wine Review #NLP #rstats
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013467804228571137">
    <blockquote placeholder><p lang="en" dir="ltr">üç∑ √ó tidytext code-through:<br>üó∫ &quot;Using French wine reviews to understand TF-IDF&quot;  ‚úçÔ∏è <a href="https://twitter.com/aleszubajak?ref_src=twsrc%5Etfw">@aleszubajak</a> <a href="https://t.co/EydHUzPSaK">https://t.co/EydHUzPSaK</a> via <a href="https://twitter.com/storybench?ref_src=twsrc%5Etfw">@storybench</a> <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <a href="https://twitter.com/hashtag/maps?src=hash&amp;ref_src=twsrc%5Etfw">#maps</a> <a href="https://t.co/Hk5WK74CfH">pic.twitter.com/Hk5WK74CfH</a></p>&mdash; Mara Averick (@dataandme) <a href="https://twitter.com/dataandme/status/1013467804228571137?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote>
</amp-twitter>

### Debugging Neural Networks
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013245064204521472">
    <blockquote placeholder><p lang="en" dir="ltr">Nice list! Our ML lab wrote up a few practical tips for debugging neural networks: <a href="https://t.co/BIURqZGnAi">https://t.co/BIURqZGnAi</a></p>&mdash; Matt Holt (@mholt6) <a href="https://twitter.com/mholt6/status/1013245064204521472?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote>
</amp-twitter>

## Bayesian inference #dataviz
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013407392342691842">
    <blockquote placeholder><p lang="en" dir="ltr">ICYMI, üí´ explorable: <br>‚ÄúBayesian inference - an interactive visualization‚Äù ‚úçÔ∏è <a href="https://twitter.com/krstoffr?ref_src=twsrc%5Etfw">@krstoffr</a> <a href="https://t.co/ViF060gCGn">https://t.co/ViF060gCGn</a>  <a href="https://twitter.com/hashtag/bayes?src=hash&amp;ref_src=twsrc%5Etfw">#bayes</a> <a href="https://twitter.com/hashtag/infovis?src=hash&amp;ref_src=twsrc%5Etfw">#infovis</a> <a href="https://t.co/V1hlQxhURg">pic.twitter.com/V1hlQxhURg</a></p>&mdash; Mara Averick (@dataandme) <a href="https://twitter.com/dataandme/status/1013407392342691842?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote>
</amp-twitter>

## Miscellaneous
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1012067290479656961">
    <blockquote placeholder><p lang="en" dir="ltr">Saw this footnote in a paper and was very sympathetic. Then scrolled up to find it was a DeepMind paper. ü§î <a href="https://t.co/Nl5IJAo9uu">pic.twitter.com/Nl5IJAo9uu</a></p>&mdash; Delip Rao (@deliprao) <a href="https://twitter.com/deliprao/status/1012067290479656961?ref_src=twsrc%5Etfw">June 27, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013563834655621120">
    <blockquote placeholder><p lang="en" dir="ltr">Hey <a href="https://twitter.com/spacy_io?ref_src=twsrc%5Etfw">@spacy_io</a> people (<a href="https://twitter.com/honnibal?ref_src=twsrc%5Etfw">@honnibal</a>, <a href="https://twitter.com/_inesmontani?ref_src=twsrc%5Etfw">@_inesmontani</a>), those speed comparisons on <a href="https://t.co/0s2zkJxrRj">https://t.co/0s2zkJxrRj</a> are not only outdated‚Äîas you note‚Äîbut the speed for the Stanford Tokenizer is just way wrong. Time to take them down? Here are our measurements: <a href="https://t.co/XAoVAFUAAJ">https://t.co/XAoVAFUAAJ</a> <a href="https://twitter.com/hashtag/NLProc?src=hash&amp;ref_src=twsrc%5Etfw">#NLProc</a></p>&mdash; Christopher Manning (@chrmanning) <a href="https://twitter.com/chrmanning/status/1013563834655621120?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013557707914088448">
    <blockquote placeholder><p lang="en" dir="ltr">In the ML research field there have been efforts to follow OSS model and attract people outside of academic institutions to participate. For example, in &quot;AI-ON&quot; there&#39;s serious projects there proposed by Google Brain, MILA, etc., looking for collaboration: <a href="https://t.co/TxriH0nwXd">https://t.co/TxriH0nwXd</a></p>&mdash; hardmaru (@hardmaru) <a href="https://twitter.com/hardmaru/status/1013557707914088448?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013588731889094658">
    <blockquote placeholder><p lang="en" dir="ltr">Joshua: Long long ago, was also grad student in statistics at Stanford (BS,MS). Brad Efron and I were in  medschool stat consulting seminar. Medical researcher came in seeking stat significance for n = 4 dogs. My mentor Lincoln Moses said ‚ÄúOnly if one of them turned into a cat‚Äù <a href="https://t.co/TI2YgOrzQm">https://t.co/TI2YgOrzQm</a></p>&mdash; Edward Tufte (@EdwardTufte) <a href="https://twitter.com/EdwardTufte/status/1013588731889094658?ref_src=twsrc%5Etfw">July 2, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013444823318237185">
    <blockquote placeholder><p lang="en" dir="ltr">Here‚Äôs what one <a href="https://twitter.com/hashtag/statistician?src=hash&amp;ref_src=twsrc%5Etfw">#statistician</a> at <a href="https://twitter.com/DataCamp?ref_src=twsrc%5Etfw">@datacamp</a> loves about <a href="https://twitter.com/hashtag/statistics?src=hash&amp;ref_src=twsrc%5Etfw">#statistics</a> <a href="https://t.co/PABDct0F5x">https://t.co/PABDct0F5x</a> <a href="https://twitter.com/drob?ref_src=twsrc%5Etfw">@drob</a> <a href="https://t.co/PRx7EJgTTA">pic.twitter.com/PRx7EJgTTA</a></p>&mdash; This is Statistics (@ThisisStats) <a href="https://twitter.com/ThisisStats/status/1013444823318237185?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote>
</amp-twitter>

### Automatic Essay Grading
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013286702389944320">
    <blockquote placeholder><p lang="en" dir="ltr">This is not how AI works, or how writing works, or how education works. <a href="https://t.co/KBPnzkRTpU">https://t.co/KBPnzkRTpU</a></p>&mdash; Ethan Plaut (@ethanplaut) <a href="https://twitter.com/ethanplaut/status/1013286702389944320?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1013556598348713984">
    <blockquote placeholder><p lang="en" dir="ltr">Automated essay grading that takes into account factual accuracy and content coherence is several years away. In the meantime, NLP and AI researchers not paid by Pearson should push back against school systems that rely on this deeply flawed technology for standardized testing. <a href="https://t.co/cJnUZRRGwR">https://t.co/cJnUZRRGwR</a></p>&mdash; James Bradbury (@jekbradbury) <a href="https://twitter.com/jekbradbury/status/1013556598348713984?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote>
</amp-twitter>
