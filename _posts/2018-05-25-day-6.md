---
title: Day 6
date: 2018-05-25 00:00:00 +00:00
layout: post
description: GDPR Day
---

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="999868631189217282">
    <blockquote placeholder><p lang="en" dir="ltr">Happy GDPR day everyone! May all your data science be compliant.</p>&mdash; Michael Dewar (@mikedewar) <a href="https://twitter.com/mikedewar/status/999868631189217282?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

## Gym Retro
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1000047804796977153">
    <blockquote placeholder><p lang="en" dir="ltr">Releasing Gym Retro — 1000+ games for reinforcement learning research, plus an integrator tool to add your own classic games. <a href="https://t.co/V5unhRknY4">https://t.co/V5unhRknY4</a> <a href="https://t.co/kRbtvsOubi">pic.twitter.com/kRbtvsOubi</a></p>&mdash; OpenAI (@OpenAI) <a href="https://twitter.com/OpenAI/status/1000047804796977153?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1000153848172855296">
    <blockquote placeholder><p lang="en" dir="ltr">OpenAI released Gym Retro. Supporting potentially over 1000 environments, will be useful for studying generalization. Should be more stable compared to Universe ... <a href="https://t.co/oOIdjvDVij">https://t.co/oOIdjvDVij</a> <a href="https://t.co/2U5O6xMjSL">pic.twitter.com/2U5O6xMjSL</a></p>&mdash; hardmaru (@hardmaru) <a href="https://twitter.com/hardmaru/status/1000153848172855296?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

## Keras MXNet Backend
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="999435047408058369">
    <blockquote placeholder><p lang="en" dir="ltr">“Keras gets a speedy new backend with keras-mxnet” by Aaron Markham <a href="https://t.co/zRZmxTKHGn">https://t.co/zRZmxTKHGn</a></p>&mdash; SandeepKrishnamurthy (@skm4ml) <a href="https://twitter.com/skm4ml/status/999435047408058369?ref_src=twsrc%5Etfw">May 23, 2018</a></blockquote>
</amp-twitter>

## Bernoulli data
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="999985452101898240"
             data-conversation="none">
    <blockquote placeholder><p lang="en" dir="ltr">Lots of people see a picture like this, which shows how the standard deviations of Bernoulli data go down as the probabilities head towards 0 or 1, and think &quot;inference will be easier for those cases.&quot; <a href="https://t.co/3WqDMhE1kZ">pic.twitter.com/3WqDMhE1kZ</a></p>&mdash; John Myles White (@johnmyleswhite) <a href="https://twitter.com/johnmyleswhite/status/999985452101898240?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="999985453729243137"
             data-conversation="none">
    <blockquote placeholder><p lang="en" dir="ltr">But if you&#39;re using the Central Limit Theorem to work with this kind of data, this other plot of the skewness of Bernoulli data over the same range should remind you that even while one aspect gets better, another aspect gets worse. <a href="https://t.co/uebABhYxmu">pic.twitter.com/uebABhYxmu</a></p>&mdash; John Myles White (@johnmyleswhite) <a href="https://twitter.com/johnmyleswhite/status/999985453729243137?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="999985768805396480"
             data-conversation="none">
    <blockquote placeholder><p lang="en" dir="ltr">also the c.v. is higher near zero and one, so need more samples to get same % accuracy in parameter estimate.</p>&mdash; Cian O&#39;Donnell (@cian_neuro) <a href="https://twitter.com/cian_neuro/status/999985768805396480?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

## Price of Lego Toys
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1000143433384816646">
    <blockquote placeholder><p lang="en" dir="ltr">This week&#39;s <a href="https://twitter.com/hashtag/KernelAwards?src=hash&amp;ref_src=twsrc%5Etfw">#KernelAwards</a> winner uses a generalized boosted regression model to predict the price of Lego toys: <a href="https://t.co/XAJ1VfO2ls">https://t.co/XAJ1VfO2ls</a> <a href="https://t.co/td52H01JVS">pic.twitter.com/td52H01JVS</a></p>&mdash; Kaggle (@kaggle) <a href="https://twitter.com/kaggle/status/1000143433384816646?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

## Calling out Andrew Ng
(Part of a long thread. Please click on the tweet to read the full thread.)
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="999772626737221632">
    <blockquote placeholder><p lang="en" dir="ltr">And how can the public trust scientists if time and time again they are presented with hype instead of science? /5</p>&mdash; Lior Pachter (@lpachter) <a href="https://twitter.com/lpachter/status/999772626737221632?ref_src=twsrc%5Etfw">May 24, 2018</a></blockquote>
</amp-twitter>

## Notables
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1000137890905673728">
    <blockquote placeholder><p lang="en" dir="ltr">As a non-native English speaker, I&#39;m always interested in studies that seek to better understand l2 acquisition. This paper does a great job. It shows that cognates (words with the same etymological origin) influence the English of non-native speakers. <a href="https://t.co/WebVtLFoWj">https://t.co/WebVtLFoWj</a></p>&mdash; Sebastian Ruder (@seb_ruder) <a href="https://twitter.com/seb_ruder/status/1000137890905673728?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1000030284484694017">
    <blockquote placeholder><p lang="en" dir="ltr">This is a super useful paper that we need more of: Better ImageNet models are not necessarily better feature extractors (ResNet is best); but for fine-tuning, ImageNet performance is strongly correlated with downstream performance.   <a href="https://t.co/MrkX4yYgHn">https://t.co/MrkX4yYgHn</a></p>&mdash; Sebastian Ruder (@seb_ruder) <a href="https://twitter.com/seb_ruder/status/1000030284484694017?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="999935451589885952">
    <blockquote placeholder><p lang="en" dir="ltr">AutoAugment: Learning Augmentation Policies from Data. They apply automated search to find data augmentation strategies that perform well on the validation set, and achieve SOTA test errors of 1.48% on CIFAR10, 10.69% on CIFAR100, 16.46% on ImageNet Top-1. <a href="https://t.co/2zRPSY2fdz">https://t.co/2zRPSY2fdz</a> <a href="https://t.co/es5yLtRrvl">pic.twitter.com/es5yLtRrvl</a></p>&mdash; hardmaru (@hardmaru) <a href="https://twitter.com/hardmaru/status/999935451589885952?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="999699350669352965">
    <blockquote placeholder><p lang="en" dir="ltr">My paper on stochastic phenomena now out in <a href="https://twitter.com/Ecology_Letters?ref_src=twsrc%5Etfw">@Ecology_Letters</a>! You can also explore the code appendix in an interactive <a href="https://twitter.com/rstudio?ref_src=twsrc%5Etfw">@RStudio</a> session using <a href="https://twitter.com/mybinderteam?ref_src=twsrc%5Etfw">@mybinderteam</a>. Check out the binder in the compendium: <a href="https://t.co/mMLOpeSFxs">https://t.co/mMLOpeSFxs</a> <a href="https://t.co/6MqgVEciY3">https://t.co/6MqgVEciY3</a></p>&mdash; Carl Boettiger (@cboettig) <a href="https://twitter.com/cboettig/status/999699350669352965?ref_src=twsrc%5Etfw">May 24, 2018</a></blockquote>
</amp-twitter>

## Miscellaneous
<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1000088368733093889">
    <blockquote placeholder><p lang="en" dir="ltr">50+ Useful <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> &amp; Prediction APIs, 2018 Edition <a href="https://t.co/m7nI39OGj3">https://t.co/m7nI39OGj3</a> <a href="https://t.co/zMSPeLmM2P">pic.twitter.com/zMSPeLmM2P</a></p>&mdash; KDnuggets (@kdnuggets) <a href="https://twitter.com/kdnuggets/status/1000088368733093889?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1000100735474954240">
    <blockquote placeholder><p lang="en" dir="ltr">ImageAI - A python library built to empower developers to build applications and systems with self-contained Computer Vision capabilities <a href="https://t.co/nYwdiFFrXW">https://t.co/nYwdiFFrXW</a></p>&mdash; Python Trending (@pythontrending) <a href="https://twitter.com/pythontrending/status/1000100735474954240?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="999922302555860992">
    <blockquote placeholder><p lang="en" dir="ltr">Things to keep in mind when reading research papers:<br>-papers are biased towards using complex models<br>-papers from well-funded labs are biased towards using the biggest datasets on the biggest machines<br>-papers from high-profile orgs don&#39;t have inherently better ideas (but more PR)</p>&mdash; Thomas Wolf (@Thom_Wolf) <a href="https://twitter.com/Thom_Wolf/status/999922302555860992?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="1000199554787049472">
    <blockquote placeholder><p lang="en" dir="ltr">Really interesting post on open source development regarding great contributions from volunteers vs sustained, focussed attention, &quot;many problems where forty people each putting in one hour a week are helpless, but that can easily be solved by one person working forty hours. &quot; <a href="https://t.co/NeaTHTKb8D">https://t.co/NeaTHTKb8D</a></p>&mdash; Sebastian Raschka (@rasbt) <a href="https://twitter.com/rasbt/status/1000199554787049472?ref_src=twsrc%5Etfw">May 26, 2018</a></blockquote>
</amp-twitter>

<amp-twitter width="400" height="400"
             layout="responsive"
             data-tweetid="999937821933649920">
    <blockquote placeholder><p lang="en" dir="ltr">“Unobtrusive features inside train stations are designed to unconsciously manipulate passenger behavior, via light, sound, and other means. Japan’s boundless creativity in this realm reflects the deep consideration given to public transportation.” <a href="https://t.co/DT7Fe1Poa3">https://t.co/DT7Fe1Poa3</a></p>&mdash; hardmaru (@hardmaru) <a href="https://twitter.com/hardmaru/status/999937821933649920?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
</amp-twitter>
